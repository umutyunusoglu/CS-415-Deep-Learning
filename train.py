import glob
import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import pretty_midi
import matplotlib.pyplot as plt
import IPython.display as ipd
import torchaudio
from tqdm import tqdm
import scipy.io.wavfile as wavfile
import yaml

# Custom Imports
from Data.dataset import SlakhTranscriptionDataset
from model import TranscriptionNet

# ==========================================
# 1. CONFIGURATION
# ==========================================
CONFIG = {
    "raw_data_dir": r"D:\Ana\Projeler\CS 415\CS-415-Deep-Learning\Data\slakh\dataset",  # Ham verinin yolu
    "root_dir": r"D:\Ana\Projeler\CS 415\CS-415-Deep-Learning\Data\slakh\processed",
    "save_path": "model_piano.pth",
    "target_class": "Piano",
    "sequence_length": 128,
    "batch_size": 32,  # Transformer iÃ§in uygun
    # --- KRÄ°TÄ°K DEÄÄ°ÅÄ°KLÄ°KLER ---
    "learning_rate": 0.0003,  # 0.005'ten 0.0001'e dÃ¼ÅŸÃ¼rdÃ¼k (En Ã¶nemli dÃ¼zeltme)
    "pos_weight": 5.0,  # 10.0 Ã§ok agresifti, 1.0 yaparak dengeledik.
    "epochs": 100,  # DÃ¼ÅŸÃ¼k learning rate ile Ã¶ÄŸrenmesi iÃ§in epoch artmalÄ± (5 yetmez)
    # ---------------------------
    "threshold": 0.3,
    "num_workers": 4,
    "sample_rate": 16000,
    "hop_length": 512,
    "split": "train",
    "device": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
}
print(f"ğŸš€ Running on device: {CONFIG['device']}")


# ==========================================
# 2. PREPROCESSING (ONE TIME RUN)
# ==========================================
def preprocess_dataset():
    input_dir = os.path.join(CONFIG["raw_data_dir"], CONFIG["split"])
    output_dir = CONFIG["root_dir"]
    os.makedirs(output_dir, exist_ok=True)

    tracks = sorted(glob.glob(os.path.join(input_dir, "Track*")))
    print(f"ğŸ”„ Preprocessing: {len(tracks)} parÃ§a iÅŸlenecek -> {output_dir}")

    if len(tracks) == 0:
        print("âŒ Hata: Raw veri bulunamadÄ±. raw_data_dir yolunu kontrol et!")
        return

    for track_path in tqdm(tracks):
        track_name = os.path.basename(track_path)
        save_path = os.path.join(output_dir, track_name + ".pt")

        if os.path.exists(save_path):
            continue

        # Ses YÃ¼kle
        mix_path = os.path.join(track_path, "mix.flac")
        if not os.path.exists(mix_path):
            continue

        waveform, sr = torchaudio.load(mix_path)
        if sr != CONFIG["sample_rate"]:
            resampler = torchaudio.transforms.Resample(sr, CONFIG["sample_rate"])
            waveform = resampler(waveform)
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        # MIDI YÃ¼kle
        meta_path = os.path.join(track_path, "metadata.yaml")
        with open(meta_path, "r") as f:
            meta = yaml.safe_load(f)

        fs = CONFIG["sample_rate"] / CONFIG["hop_length"]
        total_frames = int(waveform.shape[1] / CONFIG["hop_length"])
        piano_roll_combined = np.zeros((88, total_frames), dtype=np.float32)

        for stem_key, info in meta["stems"].items():
            if CONFIG["target_class"] == "All":
                mid_path = os.path.join(track_path, "MIDI", f"{stem_key}.mid")
                if os.path.exists(mid_path):
                    try:
                        pm = pretty_midi.PrettyMIDI(mid_path)
                        pr = pm.get_piano_roll(fs=fs)
                        pr = pr[21:109, :]  # 88 tuÅŸ
                        common_len = min(pr.shape[1], piano_roll_combined.shape[1])
                        piano_roll_combined[:, :common_len] += pr[:, :common_len]
                    except:
                        pass

        piano_roll_combined = (piano_roll_combined > 0).astype(np.float32)
        target = torch.from_numpy(piano_roll_combined).unsqueeze(0)

        torch.save(
            {
                "waveform": waveform.clone(),  # Float16 tasarruf
                "target": target.clone().bool(),  # Bool tasarruf
            },
            save_path,
        )


# ==========================================
# 3. TRAINING LOOP
# ==========================================
def train_model():
    print(f"ğŸš€ Training BaÅŸlÄ±yor: {CONFIG['device']} (TF32 + Scheduler)")

    # --- HIZ VE KARARLILIK AYARLARI ---
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.set_float32_matmul_precision("high")

    dataset = SlakhTranscriptionDataset(
        root_dir=CONFIG["root_dir"],
        split="train",
        target_class=CONFIG["target_class"],
        sequence_length=CONFIG["sequence_length"],
    )

    loader = DataLoader(
        dataset,
        batch_size=CONFIG["batch_size"],
        shuffle=True,
        num_workers=CONFIG["num_workers"],
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=2,
    )

    if len(dataset) == 0:
        print("âŒ Hata: Dataset boÅŸ! Ã–nce preprocess modunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return None, None, False

    model = TranscriptionNet().to(CONFIG["device"])

    mel_layer = torchaudio.transforms.MelSpectrogram(
        sample_rate=CONFIG["sample_rate"],
        n_fft=2048,
        hop_length=CONFIG["hop_length"],
        n_mels=229,
    ).to(CONFIG["device"])

    # Pos Weight: 2.0 yaparak gerÃ§ek notalara biraz daha aÄŸÄ±rlÄ±k veriyoruz
    criterion = nn.BCEWithLogitsLoss(
        pos_weight=torch.tensor([2.0]).to(CONFIG["device"])
    )

    # BaÅŸlangÄ±Ã§ hÄ±zÄ±: 0.0001
    optimizer = optim.Adam(model.parameters(), lr=CONFIG["learning_rate"])

    # --- YENÄ° EKLENEN SCHEDULER ---
    # SabÄ±r (patience): 3 epoch boyunca loss dÃ¼ÅŸmezse devreye girer.
    # FaktÃ¶r (factor): 0.5 -> HÄ±zÄ± yarÄ±ya indirir (0.0001 -> 0.00005)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode="min",
        factor=0.5,
        patience=5,
    )

    best_loss = float("inf")

    for epoch in range(CONFIG["epochs"]):
        model.train()
        running_loss = 0.0
        loop = tqdm(loader, desc=f"Epoch {epoch + 1}", file=sys.stdout)

        for batch_idx, (waveform, target) in enumerate(loop):
            waveform = waveform.to(CONFIG["device"], non_blocking=True)
            target = target.to(CONFIG["device"], non_blocking=True)

            if torch.isnan(waveform).any():
                continue

            optimizer.zero_grad(set_to_none=True)

            # Forward
            spec = mel_layer(waveform)
            spec = torch.log(spec + 1e-5)

            # Normalizasyon
            mean = spec.mean(dim=(1, 2), keepdim=True)
            std = spec.std(dim=(1, 2), keepdim=True)
            spec = (spec - mean) / (std + 1e-5)

            if spec.shape[-1] > CONFIG["sequence_length"]:
                spec = spec[..., : CONFIG["sequence_length"]]

            preds = model(spec)
            loss = criterion(preds, target)

            # Backward
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
            optimizer.step()

            if torch.isnan(loss):
                continue

            running_loss += loss.item()
            loop.set_postfix(loss=loss.item())

        # --- EPOCH SONU ---
        avg_loss = running_loss / len(loader) if len(loader) > 0 else 0

        # Åu anki Learning Rate'i Ã¶ÄŸrenelim
        current_lr = optimizer.param_groups[0]["lr"]
        print(f"\tEpoch {epoch + 1} Avg Loss: {avg_loss:.4f} | LR: {current_lr:.6f}")

        # --- SCHEDULER ADIMI ---
        # Scheduler'a bu epoch'un loss deÄŸerini sÃ¶ylÃ¼yoruz.
        # EÄŸer loss iyileÅŸmediyse, LR'yi dÃ¼ÅŸÃ¼recek.
        scheduler.step(avg_loss)

        if avg_loss < best_loss and avg_loss > 0:
            print(f"\tğŸ”¥ Improved: {best_loss:.4f} -> {avg_loss:.4f}. Saving...")
            best_loss = avg_loss
            torch.save(model.state_dict(), CONFIG["save_path"])

    return model, loader, True


# ==========================================
# 5. VISUALIZATION & AUDIO GENERATION
# ==========================================


def save_audio_compatible(filename, audio_data, sample_rate):
    """
    Sesi Float32 formatÄ±ndan 16-bit PCM formatÄ±na Ã§evirir ve normalizasyon yapar.
    """
    # 1. Normalizasyon (Sesi -1 ile 1 arasÄ±na Ã§ek)
    max_val = np.abs(audio_data).max()
    if max_val > 0:
        audio_data = audio_data / max_val

    # 2. 16-bit Tam SayÄ±ya Ã‡evirme
    audio_int16 = (audio_data * 32767).astype(np.int16)

    # 3. Kaydet
    wavfile.write(filename, sample_rate, audio_int16)
    print(f"ğŸ’¾ Ses Kaydedildi: {filename}")


def piano_roll_to_pretty_midi(piano_roll, fs=31.25, program=0):
    """
    Piano Roll matrisini (88 x Zaman) alÄ±r, MIDI objesine Ã§evirir.
    fs: Saniyedeki kare sayÄ±sÄ± (Frame Rate)
    """
    notes, frames = piano_roll.shape
    pm = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(program=program)  # 0: Akustik Piyano

    # BitiÅŸleri yakalamak iÃ§in padding
    piano_roll = np.pad(piano_roll, [(0, 0), (1, 1)], "constant")
    velocity_changes = np.diff(piano_roll).T
    note_on_time = np.zeros(notes)

    for time, row in enumerate(velocity_changes):
        velocity = row
        change_indices = np.where(velocity != 0)[0]

        for note_idx in change_indices:
            if velocity[note_idx] > 0:
                note_on_time[note_idx] = time
            else:
                note_number = note_idx + 21  # MIDI numarasÄ±na Ã§evir (0 -> 21)
                start_time = note_on_time[note_idx] / fs
                end_time = time / fs

                # Nota oluÅŸtur (Velocity 100 standarttÄ±r)
                note = pretty_midi.Note(
                    velocity=100, pitch=note_number, start=start_time, end=end_time
                )
                instrument.notes.append(note)

    pm.instruments.append(instrument)
    return pm


def generate_visualization():
    print(f"ğŸš€ GÃ¶rselleÅŸtirme ve Ses Ãœretimi BaÅŸlÄ±yor...")

    # 1. Modeli YÃ¼kle
    model = TranscriptionNet().to(CONFIG["device"])
    try:
        state = torch.load(CONFIG["save_path"], map_location=CONFIG["device"])
        model.load_state_dict(state)
        print(f"âœ… Model YÃ¼klendi: {CONFIG['save_path']}")
    except:
        print("âŒ Model dosyasÄ± bulunamadÄ±, eÄŸitim yapÄ±lmamÄ±ÅŸ olabilir.")
        return

    model.eval()

    # 2. Veri YÃ¼kleyici
    dataset = SlakhTranscriptionDataset(
        root_dir=CONFIG["root_dir"],
        split="train",
        target_class=CONFIG["target_class"],
        sequence_length=CONFIG["sequence_length"],
    )

    # Piyano iÃ§eren bir Ã¶rnek bulana kadar dene (Max 20 deneme)
    loader = DataLoader(dataset, batch_size=1, shuffle=True)
    found = False

    with torch.no_grad():
        for i, (waveform, target) in enumerate(loader):
            if i > 20:
                break  # Ã‡ok aramayalÄ±m
            if target.sum() == 0:
                continue  # BoÅŸ ise geÃ§

            found = True
            print(f"ğŸ¹ Piyano iÃ§eren Ã¶rnek bulundu (Ä°terasyon: {i})")

            waveform = waveform.to(CONFIG["device"])

            # --- PREPROCESSING (EÄŸitimle Birebir AynÄ±) ---
            mel_layer = torchaudio.transforms.MelSpectrogram(
                sample_rate=CONFIG["sample_rate"],
                n_fft=2048,
                hop_length=CONFIG["hop_length"],
                n_mels=229,
            ).to(CONFIG["device"])

            spec = mel_layer(waveform)
            spec = torch.log(spec + 1e-5)

            # Normalizasyon
            mean = spec.mean(dim=(1, 2), keepdim=True)
            std = spec.std(dim=(1, 2), keepdim=True)
            spec = (spec - mean) / (std + 1e-5)

            if spec.shape[-1] > CONFIG["sequence_length"]:
                spec = spec[..., : CONFIG["sequence_length"]]

            # Tahmin
            logits = model(spec)
            probs = torch.sigmoid(logits)
            preds = (probs > CONFIG["threshold"]).float()

            # Loop'tan Ã§Ä±k
            break

    if not found:
        print("âš ï¸ UyarÄ±: Piyano iÃ§eren Ã¶rnek bulunamadÄ±, rastgele bir tane Ã§iziliyor.")

    # 3. Grafik Ã‡izimi
    spec_np = spec[0, 0].cpu().numpy()
    target_np = target[0, 0].cpu().numpy()
    probs_np = probs[0, 0].cpu().numpy()
    pred_np = preds[0, 0].cpu().numpy()

    fig, ax = plt.subplots(4, 1, figsize=(10, 12), sharex=True)
    ax[0].imshow(spec_np, aspect="auto", origin="lower", cmap="inferno")
    ax[0].set_title("Spectrogram")
    ax[1].imshow(target_np, aspect="auto", origin="lower", cmap="magma")
    ax[1].set_title("Target")
    ax[2].imshow(probs_np, aspect="auto", origin="lower", cmap="viridis")
    ax[2].set_title("Probabilities (Model Output)")
    ax[3].imshow(pred_np, aspect="auto", origin="lower", cmap="magma")
    ax[3].set_title(f"Prediction (Threshold: {CONFIG['threshold']})")

    plt.tight_layout()
    plt.savefig("output_comparison.png")
    print("âœ… Grafik Kaydedildi: output_comparison.png")

    # 4. SES ÃœRETÄ°MÄ° (AUDIO SYNTHESIS)
    print("ğŸ§ Ses DosyalarÄ± OluÅŸturuluyor...")

    # Frame Rate HesabÄ± (Ã–nemli: Yoksa ses Ã§ok hÄ±zlÄ±/yavaÅŸ Ã§alar)
    # Ã–rn: 16000 / 512 = 31.25 kare/saniye
    fs_calc = CONFIG["sample_rate"] / CONFIG["hop_length"]

    # A) GerÃ§ek Ses (Ground Truth)
    pm_true = piano_roll_to_pretty_midi(target_np, fs=fs_calc)
    audio_true = pm_true.synthesize(fs=CONFIG["sample_rate"])
    save_audio_compatible("ground_truth.wav", audio_true, CONFIG["sample_rate"])

    # B) Tahmin Edilen Ses (Prediction)
    pm_pred = piano_roll_to_pretty_midi(pred_np, fs=fs_calc)
    audio_pred = pm_pred.synthesize(fs=CONFIG["sample_rate"])
    save_audio_compatible("prediction.wav", audio_pred, CONFIG["sample_rate"])

    print(
        "\nâœ¨ Ä°ÅŸlem Tamam! 'ground_truth.wav' ve 'prediction.wav' dosyalarÄ±nÄ± dinleyebilirsin."
    )


# ==========================================
# 5. MAIN ENTRY POINT
# ==========================================
if __name__ == "__main__":
    mode = "eval"  # "preprocess", "train", veya "eval" seÃ§

    if mode == "preprocess":
        preprocess_dataset()
    elif mode == "train":
        # preprocess_dataset() # Ä°stersen burayÄ± aÃ§abilirsin
        model, loader, success = train_model()
        if success:
            generate_visualization()
    elif mode == "eval":
        generate_visualization()
